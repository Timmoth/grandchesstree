{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The grand chess tree is a public distributed effort to traverse the depths of the game of chess. The project started as a result of the enjoyment I found in the early days of working on Saplings move generator. If you're interested in the project please join the discord group Currently searching depth 12!","title":"Overview"},{"location":"dev/","text":"docker-compose up dotnet run dotnet ef migrations add migration-message dotnet ef database update dotnet ef database update --connection docker buildx create --use docker buildx build --platform linux/amd64,linux/arm64 -t aptacode/grand-chess-tree-worker:latest -t aptacode/grand-chess-tree-worker:0.0.2 -f .\\GrandChessTree.Client\\Dockerfile --push . docker buildx imagetools inspect aptacode/grand-chess-tree-worker:latest docker build -t aptacode/grand-chess-tree-api:0.0.14 . docker push aptacode/grand-chess-tree-api:0.0.14 docker-compose down && docker-compose up -d Manually update perft jobs row UPDATE public.perft_jobs AS pj SET completed_fast_tasks = sub.completed_fast_tasks, completed_full_tasks = sub.completed_full_tasks, full_task_nodes = sub.full_task_nodes, fast_task_nodes = sub.fast_task_nodes, total_tasks = sub.count, verified_tasks = sub.verified_tasks FROM ( SELECT SUM(CASE WHEN fast_task_finished_at > 0 THEN 1 ELSE 0 END) AS completed_fast_tasks, SUM(CASE WHEN full_task_finished_at > 0 THEN 1 ELSE 0 END) AS completed_full_tasks, SUM(full_task_nodes * occurrences) AS full_task_nodes, SUM(fast_task_nodes * occurrences) AS fast_task_nodes, COUNT(*) AS count, SUM(CASE WHEN full_task_nodes = fast_task_nodes THEN 1 ELSE 0 END) AS verified_tasks FROM public.perft_tasks_v3 WHERE depth = 10 AND root_position_id = 1 ) AS sub WHERE pj.id = 2; Manually update full task contributions INSERT INTO public.perft_contributions ( account_id, root_position_id, depth, full_task_nodes, completed_full_tasks, completed_fast_tasks, fast_task_nodes ) SELECT full_task_account_id AS account_id, root_position_id, depth, SUM(full_task_nodes * occurrences) AS full_task_nodes, COUNT(*) AS completed_full_tasks, 0 AS completed_fast_tasks, -- Default value since fast tasks are not in the query 0.0 AS fast_task_nodes -- Default value since fast tasks are not in the query FROM public.perft_tasks_v3 WHERE depth = <DEPTH> AND root_position_id = <ID> AND full_task_finished_at > 0 GROUP BY full_task_account_id, depth, root_position_id ON CONFLICT (account_id, root_position_id, depth) DO UPDATE SET full_task_nodes = EXCLUDED.full_task_nodes, completed_full_tasks = EXCLUDED.completed_full_tasks; Manually update fast task contributions INSERT INTO public.perft_contributions ( account_id, root_position_id, depth, fast_task_nodes, completed_fast_tasks, completed_full_tasks, full_task_nodes ) SELECT fast_task_account_id AS account_id, root_position_id, depth, SUM(fast_task_nodes * occurrences) AS fast_task_nodes, COUNT(*) AS completed_fast_tasks, 0 AS completed_full_tasks, -- Default value since fast tasks are not in the query 0.0 AS full_task_nodes -- Default value since fast tasks are not in the query FROM public.perft_tasks_v3 WHERE depth = 10 AND root_position_id = 1 AND fast_task_finished_at > 0 GROUP BY fast_task_account_id, depth, root_position_id ON CONFLICT (account_id, root_position_id, depth) DO UPDATE SET fast_task_nodes = EXCLUDED.fast_task_nodes, completed_fast_tasks = EXCLUDED.completed_fast_tasks; Manually release stalled tasks UPDATE public.perft_tasks_v3 SET full_task_started_at = 0 WHERE depth = 10 and root_position_id = 1 and full_task_finished_at = 0 UPDATE public.perft_tasks_v3 SET fast_task_started_at = 0 WHERE depth = 10 and root_position_id = 1 and fast_task_finished_at = 0 Get corrupt rows SELECT Count(*) from public.perft_tasks_v3 WHERE depth = 10 and root_position_id = 1 and full_task_finished_at > 0 and fast_task_finished_at > 0 and full_task_nodes != fast_task_nodes Get contributor summary SELECT p.completed_full_tasks AS full_tasks, p.completed_fast_tasks AS fast_tasks, a.name FROM public.perft_contributions p JOIN public.accounts a ON p.account_id = a.id WHERE p.depth = 10 ORDER BY p.completed_full_tasks DESC Get perft results as json WITH aggregated AS ( SELECT SUM(t.full_task_nodes * t.occurrences) AS nodes, SUM(t.captures * t.occurrences) AS captures, SUM(t.enpassants * t.occurrences) AS enpassants, SUM(t.castles * t.occurrences) AS castles, SUM(t.promotions * t.occurrences) AS promotions, SUM(t.direct_checks * t.occurrences) AS direct_checks, SUM(t.single_discovered_checks * t.occurrences) AS single_discovered_checks, SUM(t.direct_discovered_checks * t.occurrences) AS direct_discovered_checks, SUM(t.double_discovered_checks * t.occurrences) AS double_discovered_checks, SUM(t.direct_mates * t.occurrences) AS direct_mates, SUM(t.single_discovered_mates * t.occurrences) AS single_discovered_mates, SUM(t.direct_discovered_mates * t.occurrences) AS direct_discovered_mates, SUM(t.double_discovered_mates * t.occurrences) AS double_discovered_mates, MIN(t.full_task_finished_at) as start, MAX(t.full_task_finished_at) as end FROM public.perft_tasks_v3 t WHERE t.root_position_id = 1 AND t.depth = 10 ) SELECT row_to_json(a) FROM ( SELECT *, (direct_checks + single_discovered_checks + direct_discovered_checks + double_discovered_checks) AS total_checks, (direct_mates + single_discovered_mates + direct_discovered_mates + double_discovered_mates) AS total_mates FROM aggregated ) a; Get Contributors json WITH aggregated AS ( SELECT a.id AS id, a.name AS name, p.full_task_nodes AS nodes, p.completed_full_tasks AS tasks, p.fast_task_nodes AS fast_nodes, p.completed_fast_tasks AS fast_tasks, 0 as compute_time FROM public.perft_contributions p JOIN public.accounts a ON p.account_id = a.id WHERE p.depth = 10 ORDER BY p.completed_full_tasks DESC ) SELECT row_to_json(a) FROM ( SELECT * FROM aggregated ) a; Table Size SELECT pg_size_pretty(pg_total_relation_size('perft_tasks_v3')) AS total_table_size, pg_size_pretty(pg_relation_size('perft_tasks_v3')) AS table_size, pg_size_pretty(pg_total_relation_size('perft_tasks_v3') - pg_relation_size('perft_tasks_v3')) AS toast_size, pg_size_pretty(pg_indexes_size('perft_tasks_v3')) AS index_size FROM pg_class WHERE relname = 'perft_tasks_v3'; Size per row SELECT (pg_total_relation_size('perft_tasks_v3') + pg_indexes_size('perft_tasks_v3')) / NULLIF(reltuples, 0) AS avg_row_size_with_indexes FROM pg_class WHERE relname = 'perft_tasks_v3';","title":"Dev"},{"location":"dev/#manually-update-perft-jobs-row","text":"UPDATE public.perft_jobs AS pj SET completed_fast_tasks = sub.completed_fast_tasks, completed_full_tasks = sub.completed_full_tasks, full_task_nodes = sub.full_task_nodes, fast_task_nodes = sub.fast_task_nodes, total_tasks = sub.count, verified_tasks = sub.verified_tasks FROM ( SELECT SUM(CASE WHEN fast_task_finished_at > 0 THEN 1 ELSE 0 END) AS completed_fast_tasks, SUM(CASE WHEN full_task_finished_at > 0 THEN 1 ELSE 0 END) AS completed_full_tasks, SUM(full_task_nodes * occurrences) AS full_task_nodes, SUM(fast_task_nodes * occurrences) AS fast_task_nodes, COUNT(*) AS count, SUM(CASE WHEN full_task_nodes = fast_task_nodes THEN 1 ELSE 0 END) AS verified_tasks FROM public.perft_tasks_v3 WHERE depth = 10 AND root_position_id = 1 ) AS sub WHERE pj.id = 2;","title":"Manually update perft jobs row"},{"location":"dev/#manually-update-full-task-contributions","text":"INSERT INTO public.perft_contributions ( account_id, root_position_id, depth, full_task_nodes, completed_full_tasks, completed_fast_tasks, fast_task_nodes ) SELECT full_task_account_id AS account_id, root_position_id, depth, SUM(full_task_nodes * occurrences) AS full_task_nodes, COUNT(*) AS completed_full_tasks, 0 AS completed_fast_tasks, -- Default value since fast tasks are not in the query 0.0 AS fast_task_nodes -- Default value since fast tasks are not in the query FROM public.perft_tasks_v3 WHERE depth = <DEPTH> AND root_position_id = <ID> AND full_task_finished_at > 0 GROUP BY full_task_account_id, depth, root_position_id ON CONFLICT (account_id, root_position_id, depth) DO UPDATE SET full_task_nodes = EXCLUDED.full_task_nodes, completed_full_tasks = EXCLUDED.completed_full_tasks;","title":"Manually update full task contributions"},{"location":"dev/#manually-update-fast-task-contributions","text":"INSERT INTO public.perft_contributions ( account_id, root_position_id, depth, fast_task_nodes, completed_fast_tasks, completed_full_tasks, full_task_nodes ) SELECT fast_task_account_id AS account_id, root_position_id, depth, SUM(fast_task_nodes * occurrences) AS fast_task_nodes, COUNT(*) AS completed_fast_tasks, 0 AS completed_full_tasks, -- Default value since fast tasks are not in the query 0.0 AS full_task_nodes -- Default value since fast tasks are not in the query FROM public.perft_tasks_v3 WHERE depth = 10 AND root_position_id = 1 AND fast_task_finished_at > 0 GROUP BY fast_task_account_id, depth, root_position_id ON CONFLICT (account_id, root_position_id, depth) DO UPDATE SET fast_task_nodes = EXCLUDED.fast_task_nodes, completed_fast_tasks = EXCLUDED.completed_fast_tasks;","title":"Manually update fast task contributions"},{"location":"dev/#manually-release-stalled-tasks","text":"UPDATE public.perft_tasks_v3 SET full_task_started_at = 0 WHERE depth = 10 and root_position_id = 1 and full_task_finished_at = 0 UPDATE public.perft_tasks_v3 SET fast_task_started_at = 0 WHERE depth = 10 and root_position_id = 1 and fast_task_finished_at = 0","title":"Manually release stalled tasks"},{"location":"dev/#get-corrupt-rows","text":"SELECT Count(*) from public.perft_tasks_v3 WHERE depth = 10 and root_position_id = 1 and full_task_finished_at > 0 and fast_task_finished_at > 0 and full_task_nodes != fast_task_nodes","title":"Get corrupt rows"},{"location":"dev/#get-contributor-summary","text":"SELECT p.completed_full_tasks AS full_tasks, p.completed_fast_tasks AS fast_tasks, a.name FROM public.perft_contributions p JOIN public.accounts a ON p.account_id = a.id WHERE p.depth = 10 ORDER BY p.completed_full_tasks DESC","title":"Get contributor summary"},{"location":"dev/#get-perft-results-as-json","text":"WITH aggregated AS ( SELECT SUM(t.full_task_nodes * t.occurrences) AS nodes, SUM(t.captures * t.occurrences) AS captures, SUM(t.enpassants * t.occurrences) AS enpassants, SUM(t.castles * t.occurrences) AS castles, SUM(t.promotions * t.occurrences) AS promotions, SUM(t.direct_checks * t.occurrences) AS direct_checks, SUM(t.single_discovered_checks * t.occurrences) AS single_discovered_checks, SUM(t.direct_discovered_checks * t.occurrences) AS direct_discovered_checks, SUM(t.double_discovered_checks * t.occurrences) AS double_discovered_checks, SUM(t.direct_mates * t.occurrences) AS direct_mates, SUM(t.single_discovered_mates * t.occurrences) AS single_discovered_mates, SUM(t.direct_discovered_mates * t.occurrences) AS direct_discovered_mates, SUM(t.double_discovered_mates * t.occurrences) AS double_discovered_mates, MIN(t.full_task_finished_at) as start, MAX(t.full_task_finished_at) as end FROM public.perft_tasks_v3 t WHERE t.root_position_id = 1 AND t.depth = 10 ) SELECT row_to_json(a) FROM ( SELECT *, (direct_checks + single_discovered_checks + direct_discovered_checks + double_discovered_checks) AS total_checks, (direct_mates + single_discovered_mates + direct_discovered_mates + double_discovered_mates) AS total_mates FROM aggregated ) a;","title":"Get perft results as json"},{"location":"dev/#get-contributors-json","text":"WITH aggregated AS ( SELECT a.id AS id, a.name AS name, p.full_task_nodes AS nodes, p.completed_full_tasks AS tasks, p.fast_task_nodes AS fast_nodes, p.completed_fast_tasks AS fast_tasks, 0 as compute_time FROM public.perft_contributions p JOIN public.accounts a ON p.account_id = a.id WHERE p.depth = 10 ORDER BY p.completed_full_tasks DESC ) SELECT row_to_json(a) FROM ( SELECT * FROM aggregated ) a;","title":"Get Contributors json"},{"location":"dev/#table-size","text":"SELECT pg_size_pretty(pg_total_relation_size('perft_tasks_v3')) AS total_table_size, pg_size_pretty(pg_relation_size('perft_tasks_v3')) AS table_size, pg_size_pretty(pg_total_relation_size('perft_tasks_v3') - pg_relation_size('perft_tasks_v3')) AS toast_size, pg_size_pretty(pg_indexes_size('perft_tasks_v3')) AS index_size FROM pg_class WHERE relname = 'perft_tasks_v3'; Size per row SELECT (pg_total_relation_size('perft_tasks_v3') + pg_indexes_size('perft_tasks_v3')) / NULLIF(reltuples, 0) AS avg_row_size_with_indexes FROM pg_class WHERE relname = 'perft_tasks_v3';","title":"Table Size"},{"location":"linuxInstall/","text":"Linux install instructions Get in touch, you'll need an apikey in order to connect to the server. The easiest way is to join the discord group Download the latest client (windows, linux, mac are all supported (including ARM builds!)) wget https://github.com/Timmoth/grandchesstree/releases/download/GrandChessTree-0.0.5/GrandChessTree_linux_x64 chmod 755 GrandChessTree_linux_x64 ./GrandChessTree_linux_x64 Run the client and answer the questions with the following: api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> worker_id: 0 That's it! **Note that you should pick a number of workers that corrosponds to less then the number of threads in your system since they will all be running in parrallel. ** You can close the program at any time, but do note that any progress on incomplete tasks will be lost. To exit the program use the following q+ENTER: This will wait until each subtask is completed then save progress to be resumed next time qg+ENTER: This will stop requesting new tasks, finish the current ones then exit ctrl+c: or any other abrupt close will cause all progress on current tasks to be lost. If you want to run from source (for apple silicon you may need to) git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" Alternatively you can also run the worker in docker docker run -e api_key=\"<your_api_key>\" -e api_url=\"https://api.grandchesstree.com/\" -e workers=4 -e depth=12 aptacode/grand-chess-tree-worker Making sense of the output worker: the index of the worker nps: the number of nodes per second the worker is able to process nodes: the number of nodes that have been processed for this task so far sub_tasks: completed / total sub tasks for the workers currently assigned tasks tasks: the number of full tasks that the worker has completed fen: the position the worker is currently processing The last three lines show: The number of subtasks that have been completed and the percentage of them that were duplicates of work you've already done The number of tasks you've submitted this session, and the number of tasks that you've completed but are waiting to be submitted Computed stats - this shows the amount and rate of nodes you're processing accross all of your workers (excluding caching) Effective stats - this shows the amount and rate of nodes that your sending to the server, it will be considerably higher then the rate your system can directly run the search since it includes cached nodes. | worker | nps | nodes | sub_tasks | tasks | fen | |--------|--------|--------|-----------|-------|-----------------------------------------------------------------| | 0 | 179.3m | 23.8b | 262/417 | 16 | rnbqkbnr/1ppppp1p/6p1/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 1 | 166.5m | 42.7b | 304/586 | 12 | rnbqkbnr/1ppp1ppp/4p3/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 2 | 144.6m | 92.6b | 469/565 | 11 | 1nbqkbnr/1ppppppp/r7/p7/8/P4N2/1PPPPPPP/RNBQKB1R w KQk - 0 1 | | 3 | 193.3m | 13.4b | 76/398 | 16 | rnbqkb1r/p1pppppp/1p5n/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 4 | 180.3m | 21.6b | 244/380 | 13 | rnbqkbnr/1pppppp1/7p/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 5 | 162.7m | 49.8b | 210/550 | 15 | rnbqkbnr/1pp1pppp/8/p2p4/8/P6N/1PPPPPPP/RNBQKB1R w KQkq d6 0 1 | | 6 | 164.1m | 375.6b | 612/752 | 12 | rnbqkbnr/1pp1pppp/8/p2p4/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq d6 0 1 | | 7 | 159.2m | 19.4b | 164/418 | 14 | rnbqkbnr/1pppppp1/8/p6p/8/P6N/1PPPPPPP/RNBQKB1R w KQkq h6 0 1 | | 8 | 180m | 56.9b | 380/530 | 18 | rnbqkbnr/1pp1pppp/3p4/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 9 | 180.9m | 354.6b | 596/803 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq e6 0 1 | | 10 | 169.4m | 174.7b | 549/618 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/6P1/P7/1PPPPP1P/RNBQKBNR w KQkq e6 0 1 | | 11 | 165.6m | 39.3b | 236/588 | 16 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P6N/1PPPPPPP/RNBQKB1R w KQkq e6 0 1 | | 12 | 159.1m | 104.3b | 358/579 | 14 | rnbqkbnr/1pp1pppp/8/p2p4/7P/P7/1PPPPPP1/RNBQKBNR w KQkq d6 0 1 | | 13 | 170.4m | 14.1b | 75/436 | 14 | rnbqkb1r/p1pppppp/1p3n2/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 14 | 150.5m | 18.3b | 245/417 | 17 | r1bqkbnr/1ppppppp/n7/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 15 | 158.5m | 30.9b | 314/379 | 16 | rnbqkbnr/1pppp1pp/5p2/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 16 | 162.7m | 13.6b | 224/436 | 11 | r1bqkbnr/1ppppppp/2n5/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 17 | 154m | 21.5b | 185/418 | 11 | rnbqkbnr/1ppppp1p/8/p5p1/8/P6N/1PPPPPPP/RNBQKB1R w KQkq g6 0 1 | | 18 | 199.1m | 105.7b | 525/558 | 13 | rnbqkbnr/1pp1pppp/3p4/p7/7P/P7/1PPPPPP1/RNBQKBNR w KQkq - 0 1 | | 19 | 174.7m | 4.8b | 42/422 | 11 | rnbqkbnr/p1pppppp/8/1p6/8/P1P5/1P1PPPPP/RNBQKBNR w KQkq - 0 1 | | 20 | 196.6m | 28.9b | 190/397 | 16 | rnbqkbnr/1pppp1pp/8/p4p2/8/P6N/1PPPPPPP/RNBQKB1R w KQkq f6 0 1 | | 21 | 170.9m | 14.1b | 72/436 | 15 | r1bqkbnr/p1pppppp/1pn5/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 22 | 160.1m | 4.6b | 42/510 | 12 | rn1qkbnr/pbpppppp/1p6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 23 | 176.7m | 5b | 30/432 | 3 | rn1qkbnr/p1pppppp/bp6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | completed 159k subtasks (29% cache hits), submitted 322 tasks (0 pending) [computed stats] 39.3t nodes at 4.1bnps [effective stats] 129.6t nodes at 13.3bnps Running the toolkit git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Toolkit # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" ----The Grand Chess Tree Toolkit---- perft_test:5:512:10 The command to run a perft_test locally is: perft_test:depth:hash_mb:iterations depth: the depth to search, anything over 7 is going to take a while... hash_mb: the size in MB to allocate to the hash table iterations: the number of test iterations to run","title":"Linux Installation"},{"location":"linuxInstall/#linux-install-instructions","text":"Get in touch, you'll need an apikey in order to connect to the server. The easiest way is to join the discord group Download the latest client (windows, linux, mac are all supported (including ARM builds!)) wget https://github.com/Timmoth/grandchesstree/releases/download/GrandChessTree-0.0.5/GrandChessTree_linux_x64 chmod 755 GrandChessTree_linux_x64 ./GrandChessTree_linux_x64 Run the client and answer the questions with the following: api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> worker_id: 0 That's it! **Note that you should pick a number of workers that corrosponds to less then the number of threads in your system since they will all be running in parrallel. ** You can close the program at any time, but do note that any progress on incomplete tasks will be lost. To exit the program use the following q+ENTER: This will wait until each subtask is completed then save progress to be resumed next time qg+ENTER: This will stop requesting new tasks, finish the current ones then exit ctrl+c: or any other abrupt close will cause all progress on current tasks to be lost. If you want to run from source (for apple silicon you may need to) git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" Alternatively you can also run the worker in docker docker run -e api_key=\"<your_api_key>\" -e api_url=\"https://api.grandchesstree.com/\" -e workers=4 -e depth=12 aptacode/grand-chess-tree-worker","title":"Linux install instructions"},{"location":"linuxInstall/#making-sense-of-the-output","text":"worker: the index of the worker nps: the number of nodes per second the worker is able to process nodes: the number of nodes that have been processed for this task so far sub_tasks: completed / total sub tasks for the workers currently assigned tasks tasks: the number of full tasks that the worker has completed fen: the position the worker is currently processing The last three lines show: The number of subtasks that have been completed and the percentage of them that were duplicates of work you've already done The number of tasks you've submitted this session, and the number of tasks that you've completed but are waiting to be submitted Computed stats - this shows the amount and rate of nodes you're processing accross all of your workers (excluding caching) Effective stats - this shows the amount and rate of nodes that your sending to the server, it will be considerably higher then the rate your system can directly run the search since it includes cached nodes. | worker | nps | nodes | sub_tasks | tasks | fen | |--------|--------|--------|-----------|-------|-----------------------------------------------------------------| | 0 | 179.3m | 23.8b | 262/417 | 16 | rnbqkbnr/1ppppp1p/6p1/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 1 | 166.5m | 42.7b | 304/586 | 12 | rnbqkbnr/1ppp1ppp/4p3/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 2 | 144.6m | 92.6b | 469/565 | 11 | 1nbqkbnr/1ppppppp/r7/p7/8/P4N2/1PPPPPPP/RNBQKB1R w KQk - 0 1 | | 3 | 193.3m | 13.4b | 76/398 | 16 | rnbqkb1r/p1pppppp/1p5n/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 4 | 180.3m | 21.6b | 244/380 | 13 | rnbqkbnr/1pppppp1/7p/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 5 | 162.7m | 49.8b | 210/550 | 15 | rnbqkbnr/1pp1pppp/8/p2p4/8/P6N/1PPPPPPP/RNBQKB1R w KQkq d6 0 1 | | 6 | 164.1m | 375.6b | 612/752 | 12 | rnbqkbnr/1pp1pppp/8/p2p4/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq d6 0 1 | | 7 | 159.2m | 19.4b | 164/418 | 14 | rnbqkbnr/1pppppp1/8/p6p/8/P6N/1PPPPPPP/RNBQKB1R w KQkq h6 0 1 | | 8 | 180m | 56.9b | 380/530 | 18 | rnbqkbnr/1pp1pppp/3p4/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 9 | 180.9m | 354.6b | 596/803 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq e6 0 1 | | 10 | 169.4m | 174.7b | 549/618 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/6P1/P7/1PPPPP1P/RNBQKBNR w KQkq e6 0 1 | | 11 | 165.6m | 39.3b | 236/588 | 16 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P6N/1PPPPPPP/RNBQKB1R w KQkq e6 0 1 | | 12 | 159.1m | 104.3b | 358/579 | 14 | rnbqkbnr/1pp1pppp/8/p2p4/7P/P7/1PPPPPP1/RNBQKBNR w KQkq d6 0 1 | | 13 | 170.4m | 14.1b | 75/436 | 14 | rnbqkb1r/p1pppppp/1p3n2/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 14 | 150.5m | 18.3b | 245/417 | 17 | r1bqkbnr/1ppppppp/n7/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 15 | 158.5m | 30.9b | 314/379 | 16 | rnbqkbnr/1pppp1pp/5p2/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 16 | 162.7m | 13.6b | 224/436 | 11 | r1bqkbnr/1ppppppp/2n5/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 17 | 154m | 21.5b | 185/418 | 11 | rnbqkbnr/1ppppp1p/8/p5p1/8/P6N/1PPPPPPP/RNBQKB1R w KQkq g6 0 1 | | 18 | 199.1m | 105.7b | 525/558 | 13 | rnbqkbnr/1pp1pppp/3p4/p7/7P/P7/1PPPPPP1/RNBQKBNR w KQkq - 0 1 | | 19 | 174.7m | 4.8b | 42/422 | 11 | rnbqkbnr/p1pppppp/8/1p6/8/P1P5/1P1PPPPP/RNBQKBNR w KQkq - 0 1 | | 20 | 196.6m | 28.9b | 190/397 | 16 | rnbqkbnr/1pppp1pp/8/p4p2/8/P6N/1PPPPPPP/RNBQKB1R w KQkq f6 0 1 | | 21 | 170.9m | 14.1b | 72/436 | 15 | r1bqkbnr/p1pppppp/1pn5/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 22 | 160.1m | 4.6b | 42/510 | 12 | rn1qkbnr/pbpppppp/1p6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 23 | 176.7m | 5b | 30/432 | 3 | rn1qkbnr/p1pppppp/bp6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | completed 159k subtasks (29% cache hits), submitted 322 tasks (0 pending) [computed stats] 39.3t nodes at 4.1bnps [effective stats] 129.6t nodes at 13.3bnps","title":"Making sense of the output"},{"location":"linuxInstall/#running-the-toolkit","text":"git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Toolkit # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" ----The Grand Chess Tree Toolkit---- perft_test:5:512:10 The command to run a perft_test locally is: perft_test:depth:hash_mb:iterations depth: the depth to search, anything over 7 is going to take a while... hash_mb: the size in MB to allocate to the hash table iterations: the number of test iterations to run","title":"Running the toolkit"},{"location":"memoryConf/","text":"Memory Configuration Clone the repo git clone https://github.com/Timmoth/grandchesstree CD into shared folder and open Perft.cs cd GrandChessTree.Shared vim Perft.cd To edit the memory allocation per worker thread edit it the following line (Line 36) public static Summary* AllocateHashTable(int sizeInMb = 512) - Change the value of 512 to as much ram you want to allocate per thread Please note do not over allocate as the program will OOM CD into client and run the application cd ../GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> worker_id: 0 - That's it!","title":"Memory Configuration"},{"location":"memoryConf/#memory-configuration","text":"Clone the repo git clone https://github.com/Timmoth/grandchesstree CD into shared folder and open Perft.cs cd GrandChessTree.Shared vim Perft.cd To edit the memory allocation per worker thread edit it the following line (Line 36) public static Summary* AllocateHashTable(int sizeInMb = 512) - Change the value of 512 to as much ram you want to allocate per thread Please note do not over allocate as the program will OOM CD into client and run the application cd ../GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> worker_id: 0 - That's it!","title":"Memory Configuration"},{"location":"quickstart/","text":"Quick start Get in touch, you'll need an apikey in order to connect to the server. The easiest way is to join the discord group Download the latest client (windows, linux, mac are all supported (including ARM builds!)) Run the client and answer the questions with the following: api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> worker_id: 0 That's it! **Note that you should pick a number of workers that corrosponds to less then the number of threads in your system since they will all be running in parrallel. ** You can close the program at any time, but do note that any progress on incomplete tasks will be lost. If you want to run from source (for apple silicon you may need to) git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" Alternatively you can also run the worker in docker docker run -e api_key=\"<your_api_key>\" -e api_url=\"https://api.grandchesstree.com/\" -e workers=4 -e depth=12 aptacode/grand-chess-tree-worker Making sense of the output worker: the index of the worker nps: the number of nodes per second the worker is able to process nodes: the number of nodes that have been processed for this task so far sub_tasks: completed / total sub tasks for the workers currently assigned tasks tasks: the number of full tasks that the worker has completed fen: the position the worker is currently processing The last three lines show: The number of subtasks that have been completed and the percentage of them that were duplicates of work you've already done The number of tasks you've submitted this session, and the number of tasks that you've completed but are waiting to be submitted Computed stats - this shows the amount and rate of nodes you're processing accross all of your workers (excluding caching) Effective stats - this shows the amount and rate of nodes that your sending to the server, it will be considerably higher then the rate your system can directly run the search since it includes cached nodes. | worker | nps | nodes | sub_tasks | tasks | fen | |--------|--------|--------|-----------|-------|-----------------------------------------------------------------| | 0 | 179.3m | 23.8b | 262/417 | 16 | rnbqkbnr/1ppppp1p/6p1/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 1 | 166.5m | 42.7b | 304/586 | 12 | rnbqkbnr/1ppp1ppp/4p3/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 2 | 144.6m | 92.6b | 469/565 | 11 | 1nbqkbnr/1ppppppp/r7/p7/8/P4N2/1PPPPPPP/RNBQKB1R w KQk - 0 1 | | 3 | 193.3m | 13.4b | 76/398 | 16 | rnbqkb1r/p1pppppp/1p5n/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 4 | 180.3m | 21.6b | 244/380 | 13 | rnbqkbnr/1pppppp1/7p/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 5 | 162.7m | 49.8b | 210/550 | 15 | rnbqkbnr/1pp1pppp/8/p2p4/8/P6N/1PPPPPPP/RNBQKB1R w KQkq d6 0 1 | | 6 | 164.1m | 375.6b | 612/752 | 12 | rnbqkbnr/1pp1pppp/8/p2p4/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq d6 0 1 | | 7 | 159.2m | 19.4b | 164/418 | 14 | rnbqkbnr/1pppppp1/8/p6p/8/P6N/1PPPPPPP/RNBQKB1R w KQkq h6 0 1 | | 8 | 180m | 56.9b | 380/530 | 18 | rnbqkbnr/1pp1pppp/3p4/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 9 | 180.9m | 354.6b | 596/803 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq e6 0 1 | | 10 | 169.4m | 174.7b | 549/618 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/6P1/P7/1PPPPP1P/RNBQKBNR w KQkq e6 0 1 | | 11 | 165.6m | 39.3b | 236/588 | 16 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P6N/1PPPPPPP/RNBQKB1R w KQkq e6 0 1 | | 12 | 159.1m | 104.3b | 358/579 | 14 | rnbqkbnr/1pp1pppp/8/p2p4/7P/P7/1PPPPPP1/RNBQKBNR w KQkq d6 0 1 | | 13 | 170.4m | 14.1b | 75/436 | 14 | rnbqkb1r/p1pppppp/1p3n2/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 14 | 150.5m | 18.3b | 245/417 | 17 | r1bqkbnr/1ppppppp/n7/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 15 | 158.5m | 30.9b | 314/379 | 16 | rnbqkbnr/1pppp1pp/5p2/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 16 | 162.7m | 13.6b | 224/436 | 11 | r1bqkbnr/1ppppppp/2n5/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 17 | 154m | 21.5b | 185/418 | 11 | rnbqkbnr/1ppppp1p/8/p5p1/8/P6N/1PPPPPPP/RNBQKB1R w KQkq g6 0 1 | | 18 | 199.1m | 105.7b | 525/558 | 13 | rnbqkbnr/1pp1pppp/3p4/p7/7P/P7/1PPPPPP1/RNBQKBNR w KQkq - 0 1 | | 19 | 174.7m | 4.8b | 42/422 | 11 | rnbqkbnr/p1pppppp/8/1p6/8/P1P5/1P1PPPPP/RNBQKBNR w KQkq - 0 1 | | 20 | 196.6m | 28.9b | 190/397 | 16 | rnbqkbnr/1pppp1pp/8/p4p2/8/P6N/1PPPPPPP/RNBQKB1R w KQkq f6 0 1 | | 21 | 170.9m | 14.1b | 72/436 | 15 | r1bqkbnr/p1pppppp/1pn5/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 22 | 160.1m | 4.6b | 42/510 | 12 | rn1qkbnr/pbpppppp/1p6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 23 | 176.7m | 5b | 30/432 | 3 | rn1qkbnr/p1pppppp/bp6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | completed 159k subtasks (29% cache hits), submitted 322 tasks (0 pending) [computed stats] 39.3t nodes at 4.1bnps [effective stats] 129.6t nodes at 13.3bnps Running the toolkit git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Toolkit # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" ----The Grand Chess Tree Toolkit---- perft_test:5:512:10 The command to run a perft_test locally is: perft_test:depth:hash_mb:iterations depth: the depth to search, anything over 7 is going to take a while... hash_mb: the size in MB to allocate to the hash table iterations: the number of test iterations to run","title":"Quick start"},{"location":"quickstart/#quick-start","text":"Get in touch, you'll need an apikey in order to connect to the server. The easiest way is to join the discord group Download the latest client (windows, linux, mac are all supported (including ARM builds!)) Run the client and answer the questions with the following: api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> worker_id: 0 That's it! **Note that you should pick a number of workers that corrosponds to less then the number of threads in your system since they will all be running in parrallel. ** You can close the program at any time, but do note that any progress on incomplete tasks will be lost. If you want to run from source (for apple silicon you may need to) git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" Alternatively you can also run the worker in docker docker run -e api_key=\"<your_api_key>\" -e api_url=\"https://api.grandchesstree.com/\" -e workers=4 -e depth=12 aptacode/grand-chess-tree-worker","title":"Quick start"},{"location":"quickstart/#making-sense-of-the-output","text":"worker: the index of the worker nps: the number of nodes per second the worker is able to process nodes: the number of nodes that have been processed for this task so far sub_tasks: completed / total sub tasks for the workers currently assigned tasks tasks: the number of full tasks that the worker has completed fen: the position the worker is currently processing The last three lines show: The number of subtasks that have been completed and the percentage of them that were duplicates of work you've already done The number of tasks you've submitted this session, and the number of tasks that you've completed but are waiting to be submitted Computed stats - this shows the amount and rate of nodes you're processing accross all of your workers (excluding caching) Effective stats - this shows the amount and rate of nodes that your sending to the server, it will be considerably higher then the rate your system can directly run the search since it includes cached nodes. | worker | nps | nodes | sub_tasks | tasks | fen | |--------|--------|--------|-----------|-------|-----------------------------------------------------------------| | 0 | 179.3m | 23.8b | 262/417 | 16 | rnbqkbnr/1ppppp1p/6p1/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 1 | 166.5m | 42.7b | 304/586 | 12 | rnbqkbnr/1ppp1ppp/4p3/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 2 | 144.6m | 92.6b | 469/565 | 11 | 1nbqkbnr/1ppppppp/r7/p7/8/P4N2/1PPPPPPP/RNBQKB1R w KQk - 0 1 | | 3 | 193.3m | 13.4b | 76/398 | 16 | rnbqkb1r/p1pppppp/1p5n/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 4 | 180.3m | 21.6b | 244/380 | 13 | rnbqkbnr/1pppppp1/7p/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 5 | 162.7m | 49.8b | 210/550 | 15 | rnbqkbnr/1pp1pppp/8/p2p4/8/P6N/1PPPPPPP/RNBQKB1R w KQkq d6 0 1 | | 6 | 164.1m | 375.6b | 612/752 | 12 | rnbqkbnr/1pp1pppp/8/p2p4/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq d6 0 1 | | 7 | 159.2m | 19.4b | 164/418 | 14 | rnbqkbnr/1pppppp1/8/p6p/8/P6N/1PPPPPPP/RNBQKB1R w KQkq h6 0 1 | | 8 | 180m | 56.9b | 380/530 | 18 | rnbqkbnr/1pp1pppp/3p4/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 9 | 180.9m | 354.6b | 596/803 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq e6 0 1 | | 10 | 169.4m | 174.7b | 549/618 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/6P1/P7/1PPPPP1P/RNBQKBNR w KQkq e6 0 1 | | 11 | 165.6m | 39.3b | 236/588 | 16 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P6N/1PPPPPPP/RNBQKB1R w KQkq e6 0 1 | | 12 | 159.1m | 104.3b | 358/579 | 14 | rnbqkbnr/1pp1pppp/8/p2p4/7P/P7/1PPPPPP1/RNBQKBNR w KQkq d6 0 1 | | 13 | 170.4m | 14.1b | 75/436 | 14 | rnbqkb1r/p1pppppp/1p3n2/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 14 | 150.5m | 18.3b | 245/417 | 17 | r1bqkbnr/1ppppppp/n7/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 15 | 158.5m | 30.9b | 314/379 | 16 | rnbqkbnr/1pppp1pp/5p2/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 16 | 162.7m | 13.6b | 224/436 | 11 | r1bqkbnr/1ppppppp/2n5/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 17 | 154m | 21.5b | 185/418 | 11 | rnbqkbnr/1ppppp1p/8/p5p1/8/P6N/1PPPPPPP/RNBQKB1R w KQkq g6 0 1 | | 18 | 199.1m | 105.7b | 525/558 | 13 | rnbqkbnr/1pp1pppp/3p4/p7/7P/P7/1PPPPPP1/RNBQKBNR w KQkq - 0 1 | | 19 | 174.7m | 4.8b | 42/422 | 11 | rnbqkbnr/p1pppppp/8/1p6/8/P1P5/1P1PPPPP/RNBQKBNR w KQkq - 0 1 | | 20 | 196.6m | 28.9b | 190/397 | 16 | rnbqkbnr/1pppp1pp/8/p4p2/8/P6N/1PPPPPPP/RNBQKB1R w KQkq f6 0 1 | | 21 | 170.9m | 14.1b | 72/436 | 15 | r1bqkbnr/p1pppppp/1pn5/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 22 | 160.1m | 4.6b | 42/510 | 12 | rn1qkbnr/pbpppppp/1p6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 23 | 176.7m | 5b | 30/432 | 3 | rn1qkbnr/p1pppppp/bp6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | completed 159k subtasks (29% cache hits), submitted 322 tasks (0 pending) [computed stats] 39.3t nodes at 4.1bnps [effective stats] 129.6t nodes at 13.3bnps","title":"Making sense of the output"},{"location":"quickstart/#running-the-toolkit","text":"git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Toolkit # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" ----The Grand Chess Tree Toolkit---- perft_test:5:512:10 The command to run a perft_test locally is: perft_test:depth:hash_mb:iterations depth: the depth to search, anything over 7 is going to take a while... hash_mb: the size in MB to allocate to the hash table iterations: the number of test iterations to run","title":"Running the toolkit"},{"location":"results/","text":"Here is the table of results for the initial position up to depth 11, depth 12 coming soon! | depth | nodes | captures | enpassants | castles | promotions | direct_checks | single_discovered_checks | direct_discovered_checks | double_discovered_check | total_checks | direct_mates | single_discovered_mates | direct_discoverd_mates | double_discoverd_mates | total_mates | |-------|------------------|-----------------|--------------|---------------|-------------|----------------|--------------------------|--------------------------|-------------------------|----------------|--------------|-------------------------|------------------------|------------------------|--------------| | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 1 | 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 2 | 400 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 3 | 8902 | 34 | 0 | 0 | 0 | 12 | 0 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 0 | | 4 | 197281 | 1576 | 0 | 0 | 0 | 461 | 0 | 0 | 0 | 461 | 8 | 0 | 0 | 0 | 8 | | 5 | 4865609 | 82719 | 258 | 0 | 0 | 26998 | 6 | 0 | 0 | 27004 | 347 | 0 | 0 | 0 | 347 | | 6 | 119060324 | 2812008 | 5248 | 0 | 0 | 797896 | 329 | 46 | 0 | 798271 | 10828 | 0 | 0 | 0 | 10828 | | 7 | 3195901860 | 108329926 | 319617 | 883453 | 0 | 32648427 | 18026 | 1628 | 0 | 32668081 | 435767 | 0 | 0 | 0 | 435767 | | 8 | 84998978956 | 3523740106 | 7187977 | 23605205 | 0 | 958135303 | 847039 | 147215 | 0 | 959129557 | 9852032 | 4 | 0 | 0 | 9852036 | | 9 | 2439530234167 | 125208536153 | 319496827 | 1784356000 | 17334376 | 35653060996 | 37101713 | 5547221 | 10 | 35695709940 | 399421379 | 1869 | 768715 | 0 | 400191963 | | 10 | 69352859712417 | 4092784875884 | 7824835694 | 50908510199 | 511374376 | 1077020493859 | 1531274015 | 302900733 | 879 | 1078854669486 | 8771693969 | 598058 | 18327128 | 0 | 8790619155 | | 11 | 2097651003696806 | 142537161824567 | 313603617408 | 2641343463566 | 49560932860 | 39068470901662 | 67494850305 | 11721852393 | 57443 | 39147687661803 | 360675926605 | 60344676 | 1553739626 | 0 | 362290010907 |","title":"Results"},{"location":"technical/","text":"There are four main components to the system: Database We're using PostgreSQL to store tasks, results, accounts, and API keys, with EF Core as the ORM. Tables: accounts \u2013 Stores basic information on each user, including ID, name, email, and role. api_keys \u2013 Stores authentication information (the API key itself is stored in a hashed format). perft_items \u2013 At each depth, we split the search into 101,240 tasks. This is done by first searching to depth 4 (197,281 positions), then storing a task for each of the 101,240 unique positions reachable, along with the number of times it occurs. The results from each worker task are multiplied by the occurrences. perft_tasks \u2013 Each row corresponds to the results for a specific task at a certain depth, computed by a single user. To verify correctness, multiple entries will exist for each task/depth, contributed by different users using different hashes. API We use .NET for the API, which is containerized using Docker and deployed on a VPS. The API is fairly straightforward and allows the client to: Query for a batch of tasks (using row-level locking to ensure concurrent requests don't receive the same tasks). Submit a batch of results to be stored in the database. Query for real-time and total analytics. Query for compiled results. Note: The analytics/results endpoints use response caching, so they may take a few minutes to update. Web App We have a React/TypeScript frontend using Tailwind CSS, which provides an interface for viewing the current progress of the active search, including the leaderboard. Client The client is a .NET console app, published using GitHub Actions as a self-contained binary for each platform (Windows, Linux, macOS). The client performs the actual computations. After gathering initial configuration details (stored in ./gct_config.json ), it periodically requests batches of work from the server to ensure tasks are always available. Each task requested from the server corresponds to a unique position occurring at depth 4. The client then performs a quick search to depth 2 (cumulatively reaching depth 6) to split the task into subtasks. These subtasks are deduplicated to avoid redundant searches. The client-side subtask result table is also checked to see if a position has already been processed within the session. The remaining positions are then searched to the final depth ( desired_depth - 6 ). Each worker is assigned its own task and searches serially to the final depth, periodically updating the console with progress. Once a worker completes a task, it queues the result for submission to the API before requesting the next task. Note: Choose a number of workers that is less than the number of threads available on your system since they will all run in parallel. Search Features The search algorithm employs several common techniques used in chess engines: Zobrist hashing for efficient position lookups. PEXT lookup for sliding piece moves. Legal move generation using various masks for pinned pieces, attackers, etc. Copy-make technique for certain non-legal move generations (e.g., en passant moves). Condensed board state using bitboards for pawns, knights, bishops, rooks, queens, white occupancy, and black occupancy. Pointer-based memory allocation and aligned memory to minimize bounds checking in .NET. Stack-allocated structs to reduce garbage collection overhead. There is some inherent inconsistency when using a hash table due to potential collisions. To mitigate this, a unique method is used where the full hash is XORed with the occupancy when checking for collisions. If 12 bits from the original hash are used to look up the relevant entry, these bits are then XORed again with the occupancy so that they perform a different comparison when verifying a match.","title":"Technical overview"},{"location":"technical/#database","text":"We're using PostgreSQL to store tasks, results, accounts, and API keys, with EF Core as the ORM.","title":"Database"},{"location":"technical/#tables","text":"accounts \u2013 Stores basic information on each user, including ID, name, email, and role. api_keys \u2013 Stores authentication information (the API key itself is stored in a hashed format). perft_items \u2013 At each depth, we split the search into 101,240 tasks. This is done by first searching to depth 4 (197,281 positions), then storing a task for each of the 101,240 unique positions reachable, along with the number of times it occurs. The results from each worker task are multiplied by the occurrences. perft_tasks \u2013 Each row corresponds to the results for a specific task at a certain depth, computed by a single user. To verify correctness, multiple entries will exist for each task/depth, contributed by different users using different hashes.","title":"Tables:"},{"location":"technical/#api","text":"We use .NET for the API, which is containerized using Docker and deployed on a VPS. The API is fairly straightforward and allows the client to: Query for a batch of tasks (using row-level locking to ensure concurrent requests don't receive the same tasks). Submit a batch of results to be stored in the database. Query for real-time and total analytics. Query for compiled results. Note: The analytics/results endpoints use response caching, so they may take a few minutes to update.","title":"API"},{"location":"technical/#web-app","text":"We have a React/TypeScript frontend using Tailwind CSS, which provides an interface for viewing the current progress of the active search, including the leaderboard.","title":"Web App"},{"location":"technical/#client","text":"The client is a .NET console app, published using GitHub Actions as a self-contained binary for each platform (Windows, Linux, macOS). The client performs the actual computations. After gathering initial configuration details (stored in ./gct_config.json ), it periodically requests batches of work from the server to ensure tasks are always available. Each task requested from the server corresponds to a unique position occurring at depth 4. The client then performs a quick search to depth 2 (cumulatively reaching depth 6) to split the task into subtasks. These subtasks are deduplicated to avoid redundant searches. The client-side subtask result table is also checked to see if a position has already been processed within the session. The remaining positions are then searched to the final depth ( desired_depth - 6 ). Each worker is assigned its own task and searches serially to the final depth, periodically updating the console with progress. Once a worker completes a task, it queues the result for submission to the API before requesting the next task. Note: Choose a number of workers that is less than the number of threads available on your system since they will all run in parallel.","title":"Client"},{"location":"technical/#search-features","text":"The search algorithm employs several common techniques used in chess engines: Zobrist hashing for efficient position lookups. PEXT lookup for sliding piece moves. Legal move generation using various masks for pinned pieces, attackers, etc. Copy-make technique for certain non-legal move generations (e.g., en passant moves). Condensed board state using bitboards for pawns, knights, bishops, rooks, queens, white occupancy, and black occupancy. Pointer-based memory allocation and aligned memory to minimize bounds checking in .NET. Stack-allocated structs to reduce garbage collection overhead. There is some inherent inconsistency when using a hash table due to potential collisions. To mitigate this, a unique method is used where the full hash is XORed with the occupancy when checking for collisions. If 12 bits from the original hash are used to look up the relevant entry, these bits are then XORed again with the occupancy so that they perform a different comparison when verifying a match.","title":"Search Features"}]}